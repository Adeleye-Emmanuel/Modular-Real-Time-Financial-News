{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fc98c2-b6af-48e5-afab-d0c955cb0d49",
   "metadata": {},
   "source": [
    "## **MODULAR REAL TIME MODELING FOR FINANCIAL INSIGHT GENERATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2aebe43c-a991-47a3-a3b6-3fc95409c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/emmanueladeleye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import cohere\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "\n",
    "import praw\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8750fc42-dc5c-45ea-963f-efcf11c01feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "news_apikey = os.getenv(\"NEWS_API_KEY\")\n",
    "alpha_vantage_apikey = os.getenv(\"ALPHA_VANTAGE_API_KEY\")\n",
    "openai_api = os.getenv(\"openai_api\")\n",
    "client_secret = os.getenv(\"client_secret\")\n",
    "client_id = os.getenv(\"client_id\")\n",
    "user_agent = os.getenv(\"user_agent\")\n",
    "cohere_api = os.getenv(\"cohere_api\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e949741-d6b8-46af-8c45-54ccc9b55767",
   "metadata": {},
   "source": [
    "#### **NewsAPI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "498150b3-0879-44bc-b549-06977811f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_results_napi(content):\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    content = re.sub(r'\\[\\+\\d+ chars\\]',\"\",content)    \n",
    "    content = '\\n'.join(line.strip() for line in content.splitlines() if line.strip())\n",
    "\n",
    "    return content.strip()\n",
    "\n",
    "def fetch_newsapi(query, num_articles, content_type):\n",
    "    # The function takes in the users query they intend to analyse for, the number of articles and the content type, that is either full, partial or none\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\":query,\n",
    "        \"apiKey\":news_apikey,\n",
    "        \"pageSize\":num_articles,\n",
    "        \"sortBy\":\"publishedAt\",\n",
    "        \"content\":content_type\n",
    "    }\n",
    "    response = requests.get(url, params=params).json()\n",
    "    articles = response.get(\"articles\", [])\n",
    "    parsed_articles = [\n",
    "        {\n",
    "        \"title\": a[\"title\"], \n",
    "        \"content\":a.get(\"content\") or a.get(\"description\", \"\"), # Fallback \n",
    "        \"source\": \"NewsAPI\", \n",
    "        \"date\": a[\"publishedAt\"]\n",
    "    } \n",
    "            for a in articles]\n",
    "    \n",
    "    df = pd.DataFrame(parsed_articles)\n",
    "    df['content'] = df['content'].apply(clean_results_napi)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7586f6-4a51-4eb1-b393-d7815e2906f6",
   "metadata": {},
   "source": [
    "#### **Alphavantage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b429ea9-8863-4e71-bb08-5aca56a2964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_alphavantage(query_tickers):\n",
    "    # The function takes in the users query they intend to analyse for, the number of articles and the content type, that is either full, partial or none\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\":\"NEWS_SENTIMENT\",\n",
    "        \"tickers\":query_tickers,\n",
    "        \"apikey\":alpha_vantage_apikey,\n",
    "        \"sort\": \"LATEST\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status() # this will raise http errors\n",
    "        data = response.json()\n",
    "        articles = data.get(\"feed\", [])\n",
    "\n",
    "        \n",
    "        parsed_articles = []\n",
    "        for article in articles:\n",
    "            # Parse date from \"20250121T120000\" to datetime\n",
    "            pub_date = datetime.strptime(article['time_published'], \"%Y%m%dT%H%M%S\")\n",
    "\n",
    "            # Getting sentiment scores for the queried tickers(s)\n",
    "            ticker_sentiments = [\n",
    "                ts for ts in article.get(\"ticker_sentiment\", [])\n",
    "                if ts['ticker'] in query_tickers.split(\",\")\n",
    "            ]\n",
    "\n",
    "            parsed_articles.append({\n",
    "                \"title\":article['title'],\n",
    "                \"content\":article.get(\"summary\", \"\"),\n",
    "                \"source\":\"AlphaVantage\",\n",
    "                \"date\":pub_date,\n",
    "                \"url\":article.get(\"url\", \"\"),\n",
    "                \"sentiment_label\":article.get(\"overall_sentiment_label\", \"neutral\"),\n",
    "                \"relevance_score\": float(article.get(\"relevance_score\", 0)),\n",
    "                \"ticker_sentiment\": ticker_sentiments\n",
    "                \n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(parsed_articles)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"AlphaVantage Error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fb33a-5254-43e3-96a1-b76046171dee",
   "metadata": {},
   "source": [
    "#### **RSS Feed Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e61e87b5-84f2-415c-b32b-c557f62b974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "620c6285-3288-41e2-aa17-27cd079b6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks Slip as Bull Run’s Third Year Nears Close: Markets Wrap\n",
      "European Stocks Close Best Year Since 2021 Near Record Highs\n",
      "US Signs $16 Billion of Health Agreements With African Nations\n",
      "Tycoon Elumelu Buys Stake in Biggest Nigerian Oil Explorer\n",
      "Gold and Silver Stumble at the End of Best Year Since the 1970s\n",
      "Sugar Set for Biggest Annual Drop Since 2017 on Surplus Outlook\n",
      "Cold Snap Lifts French Nuclear Output to Highest Since 2019\n",
      "Argentina Still $2.4 Billion Short for January Bond Payments\n",
      "Copper Set for Biggest Annual Gain Since 2009 on Supply Bets\n",
      "Dollar Set for Worst Year Since 2017 With Fed Drama Center Stage\n",
      "These Stocks Are the Market’s Biggest Winners and Losers in 2025\n",
      "Buying the Dips Paid Off in Stomach-Churning Markets This Year\n",
      "China Limits Foreign Beef Imports to Protect Local Farmers\n",
      "Indian Stocks Post Worst Asia Underperformance in Three Decades\n",
      "Europe Gas Heads for Sharp Annual Drop as LNG Surge Calms Market\n",
      "Oil Heads for Deepest Annual Loss Since 2020 on Surplus Concerns\n",
      "Israeli Bourse Shifts to Monday-to-Friday Trading to Woo Capital\n",
      "Taiwan Brokers Freeze Buy Orders for China Military Stocks, ETFs\n",
      "Gold Spurs Ghana’s Cedi to First Annual Gain Since at Least 1994\n",
      "SNB Refrained From Currency Interventions in Third Quarter\n",
      "Chinese AI Firms Drive Hong Kong’s Busiest IPO Month Since 2019\n",
      "China Greenlights Yuan Gains at Year-End With Measured Approach\n",
      "Spanish Stocks Notch Best Year Since 1993 as Banks Drive Rally\n",
      "CME Hikes Precious-Metal Margins Again After Price Swings\n",
      "Famed Investor Michael Burry Says He’s Not Short Tesla Shares\n",
      "Fresnillo, Burberry Shine in Best Year for UK Stocks Since 2009\n",
      "China Factory Activity Grows, Ending Longest Slump on Record\n",
      "India Imposes Three-Year Steel Tariff to Support Local Industry\n",
      "China’s Industrial Hubs to Cut Power Prices to Support Recovery\n",
      "Thailand Releases 18 Cambodian POWs as Border Ceasefire Holds\n"
     ]
    }
   ],
   "source": [
    "feed = feedparser.parse(\"https://feeds.bloomberg.com/markets/news.rss\")\n",
    "articles = []\n",
    "query = \"Tarrifs\"\n",
    "\n",
    "for entry in feed.entries:\n",
    "    print(entry.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9a53343-607d-4459-b3df-97edf3fef557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rss(feed_url, query=None):\n",
    "    \"\"\"\n",
    "    Fetches and selects articles from an RSS feed\n",
    "\n",
    "    Args:\n",
    "        feed_url (str, compulsory) and query (str, optional) - defaults to none if blank\n",
    "\n",
    "    Output:\n",
    "    Returns a list of dictionaries with article details    \n",
    "    \"\"\"\n",
    "\n",
    "    feed = feedparser.parse(feed_url)    \n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        \n",
    "        # Safely get title and content (summary/description)\n",
    "        title = entry.get('title', '')\n",
    "        summary = entry.get('summary', entry.get('description', ''))\n",
    "\n",
    "        # Checking if the query matches the title or summary\n",
    "        if query and query.lower() not in (title + summary).lower():\n",
    "            continue # this skips non-matching articles\n",
    "        \n",
    "        # Parse the publication date\n",
    "        pub_date = datetime(*entry.published_parsed[:6]) if hasattr(entry, \"published_parsed\") else None\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"content\": entry.summary,\n",
    "            \"source\": feed_url,\n",
    "            \"date\": pub_date,\n",
    "            \"url\": entry.link\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "feed_urls = [\n",
    "    \"https://feeds.bloomberg.com/technology/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/markets/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/politics/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/businessweek/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/economics/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/industries/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/bview/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/wealth/news.rss\"\n",
    "]\n",
    "\n",
    "def fetch_multiple_rss(feed_urls=feed_urls, query=None):\n",
    "\n",
    "    all_articles = []\n",
    "    for url in feed_urls:\n",
    "        articles = fetch_rss(url, query)\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7910d-90a3-4b45-8bf9-68447778e990",
   "metadata": {},
   "source": [
    "#### **Reddit Posts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "091df543-6347-4f7c-bb1a-b42287689b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c08693-fe95-4949-9ebc-14cf24575053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily Discussion Thread for December 31, 2025\n",
      "Wendy's newest hire starts tomorrow\n",
      "Stegosaurus pattern is back on SPY\n",
      "China to restrict silver exports, echoing rare earths playbook\n",
      "‪Tomorrow is Warren Buffet’s last day as CEO of Berkshire Hathaway! ‬\n"
     ]
    }
   ],
   "source": [
    "for post in reddit.subreddit(\"wallstreetbets\").hot(limit=5):\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44eeb786-0c95-4c10-8ecf-bc9fefd847a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reddit(query, percent, subreddit='wallstreetbets', limit=1000):\n",
    "    \"\"\"\n",
    "    Fetches posts from a subreddit based on a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Keyword to search for (e.g., \"Tesla\").\n",
    "        subreddit (str): Subreddit to search in (default: \"wallstreetbets\").\n",
    "        limit (int): Maximum number of posts to fetch (default: 1000).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with post details.\n",
    "    \"\"\"\n",
    "    # Initializing Reddit API client\n",
    "    reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "    sub = reddit.subreddit(subreddit)\n",
    "    posts = sub.search(query, limit=limit)\n",
    "\n",
    "    # Parsing the posts\n",
    "    parsed_posts = []\n",
    "    for post in posts:\n",
    "        parsed_posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"content\": post.selftext,\n",
    "            \"source\": f\"Reddit r/ {subreddit}\",\n",
    "            \"date\": datetime.fromtimestamp(post.created_utc),\n",
    "            \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "            \"upvotes\": post.score,\n",
    "            \"comments\": post.num_comments\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(parsed_posts)\n",
    "    sorted_df = df.sort_values(by='upvotes', ascending=False).reset_index(drop=True)\n",
    "    slice_df = sorted_df[:int(percent*len(sorted_df))]\n",
    "    return slice_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86099dbf-b77d-4b6d-bebc-1a52415810d9",
   "metadata": {},
   "source": [
    "#### **Data Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6a0c00-a4c7-4ff8-8114-6e07905c72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_pull(s_query, s_type, include_media, media_percent=None):\n",
    "\n",
    "    if include_media==True:\n",
    "        if isinstance(media_percent, float):\n",
    "            if s_type == 'general': # specifying general drops ticker based search from alphaadvantage\n",
    "                napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "                aapi_df = None\n",
    "                reddit_df = fetch_reddit(query=s_query, percent=media_percent)\n",
    "                rss_df = fetch_multiple_rss(query=s_query)\n",
    "            \n",
    "            elif s_type == 'ticker': # specifying ticker includes alphaadvantage ticker based news\n",
    "                napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "                aapi_df = fetch_alphavantage(s_query)\n",
    "                reddit_df = fetch_reddit(query=s_query, percent=0.3)\n",
    "                rss_df = fetch_multiple_rss(query=s_query)\n",
    "\n",
    "        else:\n",
    "            print('Enter a valid percentage value for media articles')\n",
    "    \n",
    "    elif include_media==False:\n",
    "        if s_type == 'general':\n",
    "            napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "            aapi_df = None\n",
    "            reddit_df = None\n",
    "            rss_df = fetch_multiple_rss(query=s_query)\n",
    "        \n",
    "        elif s_type == 'ticker':\n",
    "            napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "            aapi_df = fetch_alphavantage(s_query)\n",
    "            reddit_df = None\n",
    "            rss_df = fetch_multiple_rss(query=s_query)   \n",
    "\n",
    "    def consolidate_text(include_media, media_percent):\n",
    "        napi_df, aapi_df, reddit_df, rss_df = news_pull('stocks', 'general', include_media=True, media_percent=0.3) # including reddit news\n",
    "        dfs = [df for df in [napi_df, aapi_df, reddit_df, rss_df] if df is not None]\n",
    "        full_response = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "        full_response['full_response'] = \" Source: \" + full_response['source'] + \"\\n\" + full_response['title'] + '\\n' + full_response['content']\n",
    "        \n",
    "        texts_list = full_response['title'] + '\\n' + full_response['content']\n",
    "\n",
    "        # Stack them all into a single string\n",
    "        all_text = \"\\n\".join(texts_list)\n",
    "        return full_response, all_text\n",
    "    \n",
    "    full_response, all_text = consolidate_text(include_media, media_percent)                                  \n",
    "\n",
    "    return full_response, all_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3075b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response, all_text = news_pull('stocks', 'general', include_media=True, media_percent=0.3) # including reddit news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cea7283d-9464-4c4f-8c78-4752a92ca0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "Reddit r/ wallstreetbets                           70\n",
       "NewsAPI                                            50\n",
       "https://feeds.bloomberg.com/markets/news.rss        7\n",
       "https://feeds.bloomberg.com/technology/news.rss     3\n",
       "https://feeds.bloomberg.com/industries/news.rss     1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_response['source'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310cb581-eb47-4bc8-b7a4-45d4b020cbfe",
   "metadata": {},
   "source": [
    "#### **Processing The Text Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67294be5-8669-4c4d-9b5f-514fb7edc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning the corpus\n",
    "\n",
    "def clean_corpus(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function that is built to handle\n",
    "    - Newlines and tabs\n",
    "    - Irrelevant prefix and suffix (e.g skip comments)\n",
    "    - Javascript snippets\n",
    "    - URLs\n",
    "    - Special characters and excessive whitespace\n",
    "    - Short sentences\n",
    "    \"\"\"\n",
    "    if not isinstance(text,str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove newlines, tabs, and excessive whitespaces\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Removing javascript snippets and HTML tags\n",
    "    text = re.sub(r'{.*?}', \"\", text)\n",
    "    text = re.sub(r'href.*?\\)', \"\", text)\n",
    "    text = re.sub(r'<.*?>', \"\", text)\n",
    "\n",
    "    # Removing \"skip to comments\" and similar patterns\n",
    "    text = re.sub(r'skip to comments.*?', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Click here to view the full post.*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"click 'Accept all'.*\", '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"\"\"[^\\w\\s.,;:!?'\"-]\"\"\", '', text)\n",
    "\n",
    "    # Remove standalone single/double quotes\n",
    "    text = re.sub(r'\\s[\\'\"]\\s',' ', text)\n",
    "\n",
    "    # Remove trailing/leading whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def refine_corpus(corpus, min_length=70):\n",
    "\n",
    "    \"\"\"\n",
    "    Refining corpus by splitting into robust sentences, applying comprehensive text cleaning and filtering the corpus length\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', corpus)\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Cleaning corpus\n",
    "        clean_sent = clean_corpus(sentence)\n",
    "\n",
    "        if len(clean_sent) >= min_length:\n",
    "            # Ensuring sentences end with a proper punctuation\n",
    "            if not clean_sent.endswith(('.','?','!')):\n",
    "                clean_sent +='.'\n",
    "            cleaned_sentences.append(clean_sent)    \n",
    "    \n",
    "    return  cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f890aad0-c99a-46d3-9e63-af03abd75bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = refine_corpus(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "497a1f3e-b601-47da-8357-84593fc95e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "co = cohere.Client(cohere_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd478416-a42d-499e-af8f-f53cf099f179",
   "metadata": {},
   "source": [
    "#### **Search Option 1 - Cohere Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9345583a-2d6e-4ab0-8a56-b0f6d4417427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Embedding Shape: (136, 4096)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = co.embed(texts=cleaned_corpus, input_type='search_document').embeddings\n",
    "embeds = np.array(responses)\n",
    "print('Corpus Embedding Shape: {}\\n'.format(embeds.shape))\n",
    "dim = embeds.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.float32(embeds))\n",
    "\n",
    "def search(s_query, number_of_results):\n",
    "    query_embed = co.embed(texts=[s_query], input_type='search_query').embeddings[0]\n",
    "    distances, similar_items_ids = index.search(np.float32([query_embed]), number_of_results)\n",
    "    texts_np = np.array(cleaned_corpus)\n",
    "    results = pd.DataFrame(data={'texts':texts_np[similar_items_ids[0]], 'distance':distances[0]})\n",
    "    print(f'Query:{s_query}\\nNearest Neighbors')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ad1a619a-1281-4f8a-bfb8-02a994a06a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:Attack on Putin\n",
      "Nearest Neighbors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Someone purchased 1.3 BILLION worth of AAPL st...</td>\n",
       "      <td>8819.003906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jesus fucking Christ you absolute dolts Don't ...</td>\n",
       "      <td>9214.301758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Billionaire Investor Mark Cuban Says Reddit St...</td>\n",
       "      <td>10087.401367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>After their decades of buy blockbuster BLIAQ m...</td>\n",
       "      <td>10572.330078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This FTX situation gets more insane by the min...</td>\n",
       "      <td>10947.850586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts      distance\n",
       "0  Someone purchased 1.3 BILLION worth of AAPL st...   8819.003906\n",
       "1  Jesus fucking Christ you absolute dolts Don't ...   9214.301758\n",
       "2  Billionaire Investor Mark Cuban Says Reddit St...  10087.401367\n",
       "3  After their decades of buy blockbuster BLIAQ m...  10572.330078\n",
       "4  This FTX situation gets more insane by the min...  10947.850586"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('Attack on Putin', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb6a64-494b-4647-a339-1e7473f80aab",
   "metadata": {},
   "source": [
    "#### **Search Option 2 - BM25 + Reranking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26a6292c-0b95-4f2f-a141-1adb0bd334a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a33fd63a-69f0-4393-a2cf-63ac9930efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity + keyword match\n",
    "def bm25_tokenizer(text):\n",
    "    tokenizer_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token)>0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenizer_doc.append(token)\n",
    "\n",
    "    return tokenizer_doc\n",
    "\n",
    "def keyword_and_reranking_search(s_query, top_k=3, num_candidates=10, bm25=None):\n",
    "    print(f'Input Query: {s_query}')\n",
    "\n",
    "    #### BM25 search lexical search\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id':idx, 'score':bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(f'Top-{top_k} Lexical Search (BM25) Hits')\n",
    "\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print('\\t{:.3f}\\t{}'.format(hit['score'], cleaned_corpus[hit['corpus_id']].replace('\\n',' ')))\n",
    "\n",
    "    # Adding reranking\n",
    "    docs = [cleaned_corpus[hit['corpus_id']] for hit in bm25_hits]\n",
    "\n",
    "    print(f'\\nTop-{top_k} Hits By Rank-API ({len(bm25_hits)} BM25 Hits Re-Ranked)')\n",
    "    results = co.rerank(query=s_query, documents=docs, top_n=top_k, return_documents=True)\n",
    "\n",
    "    for hit in results.results:\n",
    "        print('\\t{:.3f}\\t{}'.format(hit.relevance_score, hit.document.text.replace('\\n','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17ecd985-e949-430a-affb-d3750f959641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [00:00<00:00, 92691.80it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for passage in tqdm(cleaned_corpus):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "342dc251-38e9-4f01-a6ba-b62e70e5076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Query: China's position on US tarrifs\n",
      "Top-3 Lexical Search (BM25) Hits\n",
      "\t0.000\t- first text file, search \"GME\", and you'll see the fails to deliver suggesting a real number in the millions, unheard of - these are purchased shares that couldn't be found.\n",
      "\t0.000\tAfter an initial uptick it will just fizzle out and you'll be left bagholding.\n",
      "\t0.000\tThey cannot cover anywhere near this price level because the shares don't exist.\n",
      "\n",
      "Top-3 Hits By Rank-API (10 BM25 Hits Re-Ranked)\n",
      "\t0.080\tTariffs to Set US Materials Up for Best Earnings in Five Years While tariffs and shaky consumer confidence continue to create headwinds for corporate America, theyre set to lift earnings growth for US materials stocks to the highest in five years.\n",
      "\t0.046\tWhen it comes time for them to pay up, expect the price to drop first.\n",
      "\t0.044\tI think the chance of seeing big upword moves is as high as its ever been.\n"
     ]
    }
   ],
   "source": [
    "keyword_and_reranking_search(s_query='China\\'s position on US tarrifs', bm25=bm25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e900c9-a86f-4cfb-a03a-a9984f2b3372",
   "metadata": {},
   "source": [
    "#### **Search Option 3 - Implementing Langchain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4e60a6e-d418-498d-b690-6b620eee2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4b63337f-b236-4033-bdcc-8066cbc64ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"key_insights\", description='3-5 bullet points summarizing key insights/outlook on the topic'),\n",
    "    ResponseSchema(name=\"key_drivers\", description='Main economic/politcal indicators driving the topic'),\n",
    "    ResponseSchema(name='risks', description='Potential risks associated with the topic'),\n",
    "    ResponseSchema(name='sentiment', description='Overall social sentiment (positive/negative/neutral with evidence) and degree of sentiment in percentage')    \n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f7b4269b-2450-4ed0-9757-f2e06f06f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(s_query, relevant_text):\n",
    "    \n",
    "    # initiate llm model\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Analyze the following news corpus regarding {query} and extract:\n",
    "    {format_instructions}\n",
    "    \n",
    "    Corpus:\n",
    "    {text}\n",
    "    \n",
    "    \"\"\")\n",
    "    #client = openai.OpenAI(api_key=openai_key)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.3,\n",
    "        openai_api_key=openai_api  # Pass key directly or use environment variable\n",
    "    )\n",
    "\n",
    "    messages = prompt.format_messages(\n",
    "        query = s_query,\n",
    "        text = relevant_text,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = llm(messages)\n",
    "    return output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4b3e0e1f-6748-478e-bb57-47dd8a65ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index(full_text):\n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "    # creating searchable index\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api)\n",
    "    return FAISS.from_texts(chunks, embeddings)    \n",
    "\n",
    "def analyze_with_semantic_search(s_query, text_list, n_results=10):\n",
    "    full_texts = \" \".join(text_list) if isinstance(text_list, list) else text_list\n",
    "    # creating vector index on full corpus\n",
    "    index = create_search_index(full_texts)\n",
    "\n",
    "    # retreiving relevant chunks\n",
    "    bm25_retriever = BM25Retriever.from_texts(full_texts)\n",
    "    faiss_retriever = index.as_retriever()\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, faiss_retriever], \n",
    "        weights=[0.4,0.6]\n",
    "    )\n",
    "    \n",
    "    relevant_text = ensemble_retriever.get_relevant_documents(s_query)\n",
    "    \n",
    "    return analyze_text(s_query, relevant_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dcc6d1-a7ee-4a1a-b88c-9eaaa023d4ff",
   "metadata": {},
   "source": [
    "#### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f6501617-8951-4602-ab08-6a1ab9f66e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lp/sq72yvgs40b2km5v8986p_g80000gn/T/ipykernel_7349/4040503641.py:10: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=openai_api)\n",
      "/var/folders/lp/sq72yvgs40b2km5v8986p_g80000gn/T/ipykernel_7349/4040503641.py:26: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  relevant_text = ensemble_retriever.get_relevant_documents(s_query)\n",
      "/var/folders/lp/sq72yvgs40b2km5v8986p_g80000gn/T/ipykernel_7349/2975228536.py:14: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(\n",
      "/var/folders/lp/sq72yvgs40b2km5v8986p_g80000gn/T/ipykernel_7349/2975228536.py:26: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_insights': '1. Russian stocks facing significant pressure and selling due to the war in Ukraine. 2. Foreign-held shares of Russian stocks may not be permitted to be sold on the Moscow Exchange. 3. Global markets experiencing volatility and uncertainty. 4. US materials stocks expected to have the best earnings in five years due to tariffs. 5. Spanish and UK stocks performing well despite geopolitical tensions.', 'key_drivers': 'Geopolitical tensions between Russia and Ukraine. Global market sentiment and uncertainty. Tariffs impacting US materials stocks. Earnings outlook for Spanish and UK stocks.', 'risks': 'Potential risk of further decline in Russian stocks. Uncertainty in global markets due to geopolitical tensions. Impact of tariffs on US materials stocks. Foreign policy risks affecting Spanish and UK stocks.', 'sentiment': 'Negative sentiment towards Russian stocks with investors unloading shares. Neutral sentiment towards global markets with volatility. Positive sentiment towards US materials stocks with expected earnings growth. Neutral sentiment towards Spanish and UK stocks with positive earnings outlook. Overall sentiment is mixed, with a slightly negative bias towards Russian stocks.'}\n"
     ]
    }
   ],
   "source": [
    "## reruning semantic search on data inclusive of reddit posts\n",
    "s_query = 'Russia Ukraine war impact on global economy'\n",
    "full_texts = cleaned_corpus\n",
    "result = analyze_with_semantic_search(s_query, full_texts, n_results=5)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "44bc407c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>key_insights</th>\n",
       "      <td>1. Russian stocks facing significant pressure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>key_drivers</th>\n",
       "      <td>Geopolitical tensions between Russia and Ukrai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>risks</th>\n",
       "      <td>Potential risk of further decline in Russian s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment</th>\n",
       "      <td>Negative sentiment towards Russian stocks with...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              0\n",
       "key_insights  1. Russian stocks facing significant pressure ...\n",
       "key_drivers   Geopolitical tensions between Russia and Ukrai...\n",
       "risks         Potential risk of further decline in Russian s...\n",
       "sentiment     Negative sentiment towards Russian stocks with..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(pd.DataFrame([result]).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45430c65-14cd-40dc-ba4a-2fb6cc7f39d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_insights': '1. War with top trade partners impacting investor risk appetite. 2. Cryptocurrency trading in line with stocks indicating uncertainty for US currency. 3. European stocks rise on hopes of tariff pause by Trump administration. 4. NFT marketplace gaining traction for trading digital items. 5. Markets reacting to trade-war risks with volatility.', 'key_drivers': \"Trade tensions with top partners, US currency stability, Trump administration's tariff policies, NFT marketplace adoption, overall market volatility.\", 'risks': 'Potential risks include prolonged trade tensions impacting global economy, currency instability, market uncertainty due to tariff policies, and volatility in digital asset trading.', 'sentiment': \"Overall sentiment is neutral with a slight negative bias due to uncertainty and market volatility. Evidence from the corpus includes phrases like 'leaving investors unwilling to take on too much risk' and 'fast-evolving tariff war'. Sentiment degree is 40% negative.\"}\n"
     ]
    }
   ],
   "source": [
    "## rerunning semantic search on data exclusive of reddit posts\n",
    "s_query_no_reddit = 'Russia Ukraine war impact on global economy'\n",
    "full_texts_no_reddit = cleaned_corpus\n",
    "result_2 = analyze_with_semantic_search(s_query_no_reddit, full_texts, n_results=5)\n",
    "\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10a1a56",
   "metadata": {},
   "source": [
    "#### **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829fa0a-8e98-4f31-a1f7-23e6dac665bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (py311_env)",
   "language": "python",
   "name": "py311_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
