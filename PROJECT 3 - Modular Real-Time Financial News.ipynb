{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6b45b5-52f6-4376-b5f0-3d9c2382fc52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting feedparser\n",
      "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sgmllib3k (from feedparser)\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hDownloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
      "Building wheels for collected packages: sgmllib3k\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6089 sha256=1895ae40e10592be68696c115da4c8566cdb178f4652fb5c12c3a829201796e9\n",
      "  Stored in directory: /Users/emmanueladeleye/Library/Caches/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
      "Successfully built sgmllib3k\n",
      "Installing collected packages: sgmllib3k, feedparser\n",
      "Successfully installed feedparser-6.0.11 sgmllib3k-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee29368a-883a-4af5-a733-300cb93ebe2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting praw\n",
      "  Downloading praw-7.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting prawcore<3,>=2.4 (from praw)\n",
      "  Downloading prawcore-2.4.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting update_checker>=0.18 (from praw)\n",
      "  Downloading update_checker-0.18.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in /opt/anaconda3/lib/python3.12/site-packages (from praw) (1.8.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from prawcore<3,>=2.4->praw) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2024.12.14)\n",
      "Downloading praw-7.8.1-py3-none-any.whl (189 kB)\n",
      "Downloading prawcore-2.4.0-py3-none-any.whl (17 kB)\n",
      "Downloading update_checker-0.18.0-py3-none-any.whl (7.0 kB)\n",
      "Installing collected packages: update_checker, prawcore, praw\n",
      "Successfully installed praw-7.8.1 prawcore-2.4.0 update_checker-0.18.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7539bfb-546a-4180-bcc2-14f92bd180a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/anaconda3/lib/python3.12/site-packages (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 781,
   "id": "2aebe43c-a991-47a3-a3b6-3fc95409c8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/emmanueladeleye/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import cohere\n",
    "import faiss\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8750fc42-dc5c-45ea-963f-efcf11c01feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "news_apikey = \"00c9eadfb2ab4d919883fb54d95aa8e0\"\n",
    "alpha_vantage_apikey = \"8I7RV41CYNQJG5SO\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e949741-d6b8-46af-8c45-54ccc9b55767",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### NewsAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "498150b3-0879-44bc-b549-06977811f956",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_results_napi(content):\n",
    "    if not content:\n",
    "        return \"\"\n",
    "\n",
    "    content = re.sub(r'\\[\\+\\d+ chars\\]',\"\",content)    \n",
    "    content = '\\n'.join(line.strip() for line in content.splitlines() if line.strip())\n",
    "\n",
    "    return content.strip()\n",
    "\n",
    "def fetch_newsapi(query, num_articles, content_type):\n",
    "    # The function takes in the users query they intend to analyse for, the number of articles and the content type, that is either full, partial or none\n",
    "    url = \"https://newsapi.org/v2/everything\"\n",
    "    params = {\n",
    "        \"q\":query,\n",
    "        \"apiKey\":news_apikey,\n",
    "        \"pageSize\":num_articles,\n",
    "        \"sortBy\":\"publishedAt\",\n",
    "        \"content\":content_type\n",
    "    }\n",
    "    response = requests.get(url, params=params).json()\n",
    "    articles = response.get(\"articles\", [])\n",
    "    parsed_articles = [\n",
    "        {\n",
    "        \"title\": a[\"title\"], \n",
    "        \"content\":a.get(\"content\") or a.get(\"description\", \"\"), # Fallback \n",
    "        \"source\": \"NewsAPI\", \n",
    "        \"date\": a[\"publishedAt\"]\n",
    "    } \n",
    "            for a in articles]\n",
    "    \n",
    "    df = pd.DataFrame(parsed_articles)\n",
    "    df['content'] = df['content'].apply(clean_results_napi)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7586f6-4a51-4eb1-b393-d7815e2906f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Alphavantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "8b429ea9-8863-4e71-bb08-5aca56a2964a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_alphavantage(query_tickers):\n",
    "    # The function takes in the users query they intend to analyse for, the number of articles and the content type, that is either full, partial or none\n",
    "    url = \"https://www.alphavantage.co/query\"\n",
    "    params = {\n",
    "        \"function\":\"NEWS_SENTIMENT\",\n",
    "        \"tickers\":query_tickers,\n",
    "        \"apikey\":alpha_vantage_apikey,\n",
    "        \"sort\": \"LATEST\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, params=params)\n",
    "        response.raise_for_status() # this will raise http errors\n",
    "        data = response.json()\n",
    "        articles = data.get(\"feed\", [])\n",
    "\n",
    "        \n",
    "        parsed_articles = []\n",
    "        for article in articles:\n",
    "            # Parse date from \"20250121T120000\" to datetime\n",
    "            pub_date = datetime.strptime(article['time_published'], \"%Y%m%dT%H%M%S\")\n",
    "\n",
    "            # Getting sentiment scores for the queried tickers(s)\n",
    "            ticker_sentiments = [\n",
    "                ts for ts in article.get(\"ticker_sentiment\", [])\n",
    "                if ts['ticker'] in query_tickers.split(\",\")\n",
    "            ]\n",
    "\n",
    "            parsed_articles.append({\n",
    "                \"title\":article['title'],\n",
    "                \"content\":article.get(\"summary\", \"\"),\n",
    "                \"source\":\"AlphaVantage\",\n",
    "                \"date\":pub_date,\n",
    "                \"url\":article.get(\"url\", \"\"),\n",
    "                \"sentiment_label\":article.get(\"overall_sentiment_label\", \"neutral\"),\n",
    "                \"relevance_score\": float(article.get(\"relevance_score\", 0)),\n",
    "                \"ticker_sentiment\": ticker_sentiments\n",
    "                \n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(parsed_articles)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"AlphaVantage Error: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2fb33a-5254-43e3-96a1-b76046171dee",
   "metadata": {},
   "source": [
    "#### RSS Feed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "e61e87b5-84f2-415c-b32b-c557f62b974c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "620c6285-3288-41e2-aa17-27cd079b6f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KKR Is Said to Near Deal for OSTTRA for About $3 Billion\n",
      "Emergency Responders on Hudson River Crash Site\n",
      "Central Banks Prepare First G-7 Responses to US Chaos\n",
      "Oaktree, TCW and Sona Spot Opportunity in Market Turmoil\n",
      "Turkey Suffered Worst Cold Snap Since 2014, Minister Says\n",
      "UK MPs Pass Emergency Bill to Rescue Troubled British Steel\n",
      "Reynolds Argues for Emergency Powers Over British Steel\n",
      "US-Iran Talks, Exterior Scenes of Muscat\n",
      "Trump Wants a Sovereign Wealth Fund: Can the US Pull it Off?\n",
      "The Trump Family Is Going All-In on Crypto Projects, From Bitcoin Mining to Stablecoins\n",
      "Billionaire Pradas’ Succession Plan Backed by Versace Deal\n",
      "Peebles, McNeely to Raise Fund for Office Conversions\n",
      "Summers Says This Moment Is Very Different From Previous Trump Tariff Moment\n",
      "Summers: Trump Policies ‘Being Improvised on a Daily Basis’\n",
      "EU Should Use Frozen Russian Assets to Boost Defense, Spain Says\n",
      "Norway’s Wealth Fund Plans to Back Paschi’s Mediobanca Bid\n",
      "Italy’s Giorgetti Says No Extra Borrowing Despite Lower Growth\n",
      "Polish Finance Chief Sees Small Downside Risk to Growth Estimate\n",
      "Argentina Lands $20 Billion IMF Agreement and Eases FX Rules\n",
      "Mauritius Plans New Spending as IMF Warns on Next Budget Outlays\n",
      "Extreme Weather Hits China, Leading to Cancellation of Flights\n",
      "Trump Plans Order to Enable Critical Metals Stockpiling: FT\n",
      "Italy’s Rating Upgraded by S&amp;P in Another Win for Meloni\n",
      "Punch Drunk Traders Across Asia Ready for Another Week of Drama\n",
      "Hungary Pushed to Edge of Junk at S&amp;P on Orban Spending Plan\n",
      "<strong>Jim Chanos on Who's Getting Caught Swimming Naked</strong>\n",
      "Jim Chanos on Tariffs, AI, Private Equity, and Elon Musk\n",
      "European Firms Stockpile, Shift Production to Brace for Tariffs\n",
      "In Full: Swiss President on Trump Tariffs, UBS Bank Cap\n",
      "Trump Made Promise to Swiss Just Hours Before Suspending Tariffs\n"
     ]
    }
   ],
   "source": [
    "feed = feedparser.parse(\"https://feeds.bloomberg.com/markets/news.rss\")\n",
    "articles = []\n",
    "query = \"Tarrifs\"\n",
    "\n",
    "for entry in feed.entries:\n",
    "    print(entry.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "b9a53343-607d-4459-b3df-97edf3fef557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_rss(feed_url, query=None):\n",
    "    \"\"\"\n",
    "    Fetches and selects articles from an RSS feed\n",
    "\n",
    "    Args:\n",
    "        feed_url (str, compulsory) and query (str, optional) - defaults to none if blank\n",
    "\n",
    "    Output:\n",
    "    Returns a list of dictionaries with article details    \n",
    "    \"\"\"\n",
    "\n",
    "    feed = feedparser.parse(feed_url)    \n",
    "\n",
    "    articles = []\n",
    "\n",
    "    for entry in feed.entries:\n",
    "        \n",
    "        # Safely get title and content (summary/description)\n",
    "        title = entry.get('title', '')\n",
    "        summary = entry.get('summary', entry.get('description', ''))\n",
    "\n",
    "        # Checking if the query matches the title or summary\n",
    "        if query and query.lower() not in (title + summary).lower():\n",
    "            continue # this skips non-matching articles\n",
    "        \n",
    "        # Parse the publication date\n",
    "        pub_date = datetime(*entry.published_parsed[:6]) if hasattr(entry, \"published_parsed\") else None\n",
    "\n",
    "        articles.append({\n",
    "            \"title\": entry.title,\n",
    "            \"content\": entry.summary,\n",
    "            \"source\": feed_url,\n",
    "            \"date\": pub_date,\n",
    "            \"url\": entry.link\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "feed_urls = [\n",
    "    \"https://feeds.bloomberg.com/technology/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/markets/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/politics/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/businessweek/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/economics/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/industries/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/bview/news.rss\",\n",
    "    \"https://feeds.bloomberg.com/wealth/news.rss\"\n",
    "]\n",
    "\n",
    "def fetch_multiple_rss(feed_urls=feed_urls, query=None):\n",
    "\n",
    "    all_articles = []\n",
    "    for url in feed_urls:\n",
    "        articles = fetch_rss(url, query)\n",
    "        all_articles.extend(articles)\n",
    "\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    return df    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e7910d-90a3-4b45-8bf9-68447778e990",
   "metadata": {},
   "source": [
    "#### Reddit Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "4419491f-e51a-497b-8429-5a67a1ac0405",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2ac75e45-6452-41bc-8451-ea7b3a506c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_secret = \"Ny1QzWVtkpWONqY-yT5ylKP1qtyPeQ\"\n",
    "client_id = \"9Mem4YKphEAkPzxF2orx8g\"\n",
    "user_agent = \"SentimenTracker/1.0 Emmanuel Adeleye\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "091df543-6347-4f7c-bb1a-b42287689b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = praw.Reddit(\n",
    "    client_id=client_id,\n",
    "    client_secret=client_secret,\n",
    "    user_agent=user_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "e6c08693-fe95-4949-9ebc-14cf24575053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weekend Discussion Thread for the Weekend of April 11, 2025\n",
      "Weekly Earnings Thread 4/14 - 4/18\n",
      "Surprise my wife after last week.\n",
      "Trump exempts phones, computers and chips from “reciprocal” tariffs.\n",
      "We did it Xi\n"
     ]
    }
   ],
   "source": [
    "for post in reddit.subreddit(\"wallstreetbets\").hot(limit=5):\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "44eeb786-0c95-4c10-8ecf-bc9fefd847a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_reddit(query, percent, subreddit='wallstreetbets', limit=1000):\n",
    "    \"\"\"\n",
    "    Fetches posts from a subreddit based on a query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): Keyword to search for (e.g., \"Tesla\").\n",
    "        subreddit (str): Subreddit to search in (default: \"wallstreetbets\").\n",
    "        limit (int): Maximum number of posts to fetch (default: 1000).\n",
    "    \n",
    "    Returns:\n",
    "        List of dicts with post details.\n",
    "    \"\"\"\n",
    "    # Initializing Reddit API client\n",
    "    reddit = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "    sub = reddit.subreddit(subreddit)\n",
    "    posts = sub.search(query, limit=limit)\n",
    "\n",
    "    # Parsing the posts\n",
    "    parsed_posts = []\n",
    "    for post in posts:\n",
    "        parsed_posts.append({\n",
    "            \"title\": post.title,\n",
    "            \"content\": post.selftext,\n",
    "            \"source\": f\"Reddit r/ {subreddit}\",\n",
    "            \"date\": datetime.fromtimestamp(post.created_utc),\n",
    "            \"url\": f\"https://reddit.com{post.permalink}\",\n",
    "            \"upvotes\": post.score,\n",
    "            \"comments\": post.num_comments\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(parsed_posts)\n",
    "    sorted_df = df.sort_values(by='upvotes', ascending=False).reset_index(drop=True)\n",
    "    slice_df = sorted_df[:int(percent*len(sorted_df))]\n",
    "    return slice_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86099dbf-b77d-4b6d-bebc-1a52415810d9",
   "metadata": {},
   "source": [
    "#### Data Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 853,
   "id": "ac6a0c00-a4c7-4ff8-8114-6e07905c72d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def news_pull(s_query, s_type, include_media, media_percent=None):\n",
    "\n",
    "    if include_media==True:\n",
    "        if isinstance(media_percent, float):\n",
    "            if s_type == 'general': # specifying general drops ticker based search from alphaadvantage\n",
    "                napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "                aapi_df = None\n",
    "                reddit_df = fetch_reddit(query=s_query, percent=media_percent)\n",
    "                rss_df = fetch_multiple_rss(query=s_query)\n",
    "            \n",
    "            elif s_type == 'ticker': # specifying ticker includes alphaadvantage ticker based news\n",
    "                napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "                aapi_df = fetch_alphavantage(s_query)\n",
    "                reddit_df = fetch_reddit(query=s_query, percent=0.3)\n",
    "                rss_df = fetch_multiple_rss(query=s_query)\n",
    "\n",
    "        else:\n",
    "            print('Enter a valid percentage value for media articles')\n",
    "    \n",
    "    elif include_media==False:\n",
    "        if s_type == 'general':\n",
    "            napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "            aapi_df = None\n",
    "            reddit_df = None\n",
    "            rss_df = fetch_multiple_rss(query=s_query)\n",
    "        \n",
    "        elif s_type == 'ticker':\n",
    "            napi_df = fetch_newsapi(s_query, 50, \"full\")\n",
    "            aapi_df = fetch_alphavantage(s_query)\n",
    "            reddit_df = None\n",
    "            rss_df = fetch_multiple_rss(query=s_query)   \n",
    "            \n",
    "    return napi_df, aapi_df, reddit_df, rss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "7f70e4c2-377d-4ae5-8c63-eb3ecd4649ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "napi_df, aapi_df, reddit_df, rss_df = news_pull('stocks', 'general', include_media=False)\n",
    "#napi_df, aapi_df, reddit_df, rss_df = news_pull('stocks', 'general', include_media=True, media_percent=0.3) # including reddit news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "id": "092abc49-fab1-4e7b-a2c5-82c64817f776",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response = pd.concat([napi_df, aapi_df, reddit_df, rss_df], axis=0).reset_index().drop('index', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "0b830311-8288-49e2-a520-5cb2f18f409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_response['full_response'] = \" Source: \" + full_response['source'] + \"\\n\" + full_response['title'] + '\\n' + full_response['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "cea7283d-9464-4c4f-8c78-4752a92ca0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "Reddit r/ wallstreetbets                           71\n",
       "NewsAPI                                            50\n",
       "https://feeds.bloomberg.com/markets/news.rss        2\n",
       "https://feeds.bloomberg.com/technology/news.rss     1\n",
       "https://feeds.bloomberg.com/bview/news.rss          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 817,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_response['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "a53d1d65-6c16-4e94-a685-7c58dd08e339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "source\n",
       "NewsAPI                                            50\n",
       "https://feeds.bloomberg.com/markets/news.rss        2\n",
       "https://feeds.bloomberg.com/technology/news.rss     1\n",
       "https://feeds.bloomberg.com/bview/news.rss          1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 861,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run 2 - without reddit upvoted majority\n",
    "full_response['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "a9d7b8b8-7cfe-489c-a05a-878fa573eb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_list = full_response['title'] + '\\n' + full_response['content']\n",
    "\n",
    "# Stack them all into a single string\n",
    "all_text = \"\\n\".join(texts_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310cb581-eb47-4bc8-b7a4-45d4b020cbfe",
   "metadata": {},
   "source": [
    "### Processing The Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "67294be5-8669-4c4d-9b5f-514fb7edc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "### cleaning the corpus\n",
    "\n",
    "def clean_corpus(text):\n",
    "\n",
    "    \"\"\"\n",
    "    Comprehensive text cleaning function that is built to handle\n",
    "    - Newlines and tabs\n",
    "    - Irrelevant prefix and suffix (e.g skip comments)\n",
    "    - Javascript snippets\n",
    "    - URLs\n",
    "    - Special characters and excessive whitespace\n",
    "    - Short sentences\n",
    "    \"\"\"\n",
    "    if not isinstance(text,str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove newlines, tabs, and excessive whitespaces\n",
    "    text = \" \".join(text.split())\n",
    "\n",
    "    # Removing javascript snippets and HTML tags\n",
    "    text = re.sub(r'{.*?}', \"\", text)\n",
    "    text = re.sub(r'href.*?\\)', \"\", text)\n",
    "    text = re.sub(r'<.*?>', \"\", text)\n",
    "\n",
    "    # Removing \"skip to comments\" and similar patterns\n",
    "    text = re.sub(r'skip to comments.*?', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Click here to view the full post.*', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"click 'Accept all'.*\", '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r\"\"\"[^\\w\\s.,;:!?'\"-]\"\"\", '', text)\n",
    "\n",
    "    # Remove standalone single/double quotes\n",
    "    text = re.sub(r'\\s[\\'\"]\\s',' ', text)\n",
    "\n",
    "    # Remove trailing/leading whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def refine_corpus(corpus, min_length=70):\n",
    "\n",
    "    \"\"\"\n",
    "    Refining corpus by splitting into robust sentences, applying comprehensive text cleaning and filtering the corpus length\n",
    "    \"\"\"\n",
    "\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', corpus)\n",
    "\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        # Cleaning corpus\n",
    "        clean_sent = clean_corpus(sentence)\n",
    "\n",
    "        if len(clean_sent) >= min_length:\n",
    "            # Ensuring sentences end with a proper punctuation\n",
    "            if not clean_sent.endswith(('.','?','!')):\n",
    "                clean_sent +='.'\n",
    "            cleaned_sentences.append(clean_sent)    \n",
    "    \n",
    "    return  cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "f890aad0-c99a-46d3-9e63-af03abd75bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = refine_corpus(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "id": "497a1f3e-b601-47da-8357-84593fc95e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = 'RmXpDEEuYMWmh9pEz3aCUY9tS4KtqmyPOEY6Dvcq'\n",
    "\n",
    "co = cohere.Client(api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd478416-a42d-499e-af8f-f53cf099f179",
   "metadata": {},
   "source": [
    "#### Search Option 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "id": "9345583a-2d6e-4ab0-8a56-b0f6d4417427",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Embedding Shape: (133, 4096)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "responses = co.embed(texts=cleaned_corpus, input_type='search_document').embeddings\n",
    "embeds = np.array(responses)\n",
    "print('Corpus Embedding Shape: {}\\n'.format(embeds.shape))\n",
    "dim = embeds.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(np.float32(embeds))\n",
    "\n",
    "def search(s_query, number_of_results):\n",
    "    query_embed = co.embed(texts=[s_query], input_type='search_query').embeddings[0]\n",
    "    distances, similar_items_ids = index.search(np.float32([query_embed]), number_of_results)\n",
    "    texts_np = np.array(cleaned_corpus)\n",
    "    results = pd.DataFrame(data={'texts':texts_np[similar_items_ids[0]], 'distance':distances[0]})\n",
    "    print(f'Query:{s_query}\\nNearest Neighbors')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "ad1a619a-1281-4f8a-bfb8-02a994a06a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:Stock market\n",
      "Nearest Neighbors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texts</th>\n",
       "      <th>distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So not on Luxury Shares Drop, Price Hikes Loom...</td>\n",
       "      <td>8956.212891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dow Jones Today: Stocks Gain on Hopes for Deal...</td>\n",
       "      <td>9151.861328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>As most ASX investors watching the markets tod...</td>\n",
       "      <td>9253.870117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texts     distance\n",
       "0  So not on Luxury Shares Drop, Price Hikes Loom...  8956.212891\n",
       "1  Dow Jones Today: Stocks Gain on Hopes for Deal...  9151.861328\n",
       "2  As most ASX investors watching the markets tod...  9253.870117"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('Stock market', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb6a64-494b-4647-a339-1e7473f80aab",
   "metadata": {},
   "source": [
    "#### Search Option 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "26a6292c-0b95-4f2f-a141-1adb0bd334a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "import string\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "a33fd63a-69f0-4393-a2cf-63ac9930efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity + keyword match\n",
    "def bm25_tokenizer(text):\n",
    "    tokenizer_doc = []\n",
    "    for token in text.lower().split():\n",
    "        token = token.strip(string.punctuation)\n",
    "\n",
    "        if len(token)>0 and token not in _stop_words.ENGLISH_STOP_WORDS:\n",
    "            tokenizer_doc.append(token)\n",
    "\n",
    "    return tokenizer_doc\n",
    "\n",
    "def keyword_and_reranking_search(s_query, top_k=3, num_candidates=10):\n",
    "    print(f'Input Query: {s_query}')\n",
    "\n",
    "    #### BM25 search lexical search\n",
    "    bm25_scores = bm25.get_scores(bm25_tokenizer(query))\n",
    "    top_n = np.argpartition(bm25_scores, -num_candidates)[-num_candidates:]\n",
    "    bm25_hits = [{'corpus_id':idx, 'score':bm25_scores[idx]} for idx in top_n]\n",
    "    bm25_hits = sorted(bm25_hits, key=lambda x: x['score'], reverse=True)\n",
    "\n",
    "    print(f'Top-{top_k} Lexical Search (BM25) Hits')\n",
    "\n",
    "    for hit in bm25_hits[0:top_k]:\n",
    "        print('\\t{:.3f}\\t{}'.format(hit['score'], cleaned_corpus[hit['corpus_id']].replace('\\n',' ')))\n",
    "\n",
    "    # Adding reranking\n",
    "    docs = [cleaned_corpus[hit['corpus_id']] for hit in bm25_hits]\n",
    "\n",
    "    print(f'\\nTop-{top_k} Hits By Rank-API ({len(bm25_hits)} BM25 Hits Re-Ranked)')\n",
    "    results = co.rerank(query=s_query, documents=docs, top_n=top_k, return_documents=True)\n",
    "\n",
    "    for hit in results.results:\n",
    "        print('\\t{:.3f}\\t{}'.format(hit.relevance_score, hit.document.text.replace('\\n','')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "17ecd985-e949-430a-affb-d3750f959641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 39/39 [00:00<00:00, 26820.44it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_corpus = []\n",
    "for passage in tqdm(cleaned_corpus):\n",
    "    tokenized_corpus.append(bm25_tokenizer(passage))\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "342dc251-38e9-4f01-a6ba-b62e70e5076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Query: China's position on US tarrifs\n",
      "Top-3 Lexical Search (BM25) Hits\n",
      "\t1.495\tAd Policy A group of workers look on as President Donald Trump announces his new tarrifs plan at the White House 總統重申願零關稅談判 何志勇提醒關稅非川普目的而是武器 If you.\n",
      "\t1.442\tMembers of Generation Z are crafting their own Steve Jobs-style uniforms for the workplace, saying outfit repetition gives them a sense of control over  What could reciprocal tarrifs mean for my investments?\n",
      "\t1.193\tEU seeks unity in first strike back at Trump tariffsReuters   462025  Philip Blenkinsop Posted on 04062025 3:49:16 PM PDT by Nifty BRUSSELS, April 6 Reuters - Europe Lutnick says Trump won't delay tarrifs, Bessent says US talking with 50 countries Investing.com -- Surrogates of U.S. President Donald Trump took to the airwaves on Sunday to reassure the public of his tariff policies despite stocks seeing a brutal sell-off on Thursday and Friday. Lutnick says Trump won't delay tarrifs, Bessent says US talking with 50 countries , 200; return false;\"gt;Why are auto supply chains shutting down stores?\n",
      "\n",
      "Top-3 Hits By Rank-API (10 BM25 Hits Re-Ranked)\n",
      "\t0.871\tFollowup fallingcow:Companies are gonna be really, really reluctant to break ground on big facilities with break-even points five years or more out though, when there's little certainty that these policies wi SP 500 Opens 2.4 Lower After China's Response To Trump Tarrifs New York: Wall Street stocks opened sharply lower Friday, shrugging off solid US hiring data and extending a rout after China announced hefty tariffs in response to President Donald Trump's levies.\n",
      "\t0.762\tS Beijing consumers mull spending habits as tariffs kick in Chinese consumers say they are worried about the impact of US tariffs - Copyright AFP Hector RETAMAL Chinese consumers in Beijing mulled their their spending habits and said they are prepared to for Market outlook : RBI पलस क दन लल नशन म बद हआ बजर, जनए 10 अपरल क कस रह सकत ह इसक चल Stock market : 9 22500 379.93 0.51 73,847.15 136.70 0.61 22,399.15 1500 , 2241 138 , , , , , , , 0.8 1 0.3  1.5  , 2 2026 4 , , \" 25 , Trump tarrifs : US , - 22,300-22,250 22, Market outlook : RBI पलस क दन लल नशन म बद हआ बजर, जनए 11 अपरल क कस रह सकत ह इसक चल Stock market : 9 22500 379.93 0.51 73,847.15 136.70 0.61 22,399.15 1500 , 2241 138 , , , , , , , 0.8 1 0.3  1.5  , 2 10 2026 4 , , \" 25 , Trump tarrifs : US , - 22,300-22,2 Hannover Messe 2025: Mind The Reality Gap Last week I joined 127,000 of our closest friends in Germany for the Hannover Messe trade fair, which once again showcased all thats new and interesting in the smart manufacturing world.\n",
      "\t0.547\tAssuming stinky imposes x tariffs and midnight tonight, does it apply to stuff ordered, already en route, or even now unloaded in US ports 'Way cheaper' to make chocolate bars not in America amid tarrifs, MrBeast says , 200; return false;\"gt;Why did tariffs spark market volatility?\n"
     ]
    }
   ],
   "source": [
    "keyword_and_reranking_search(s_query='China\\'s position on US tarrifs')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e900c9-a86f-4cfb-a03a-a9984f2b3372",
   "metadata": {},
   "source": [
    "#### Search Option 3 - Implementing Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "e4e60a6e-d418-498d-b690-6b620eee2718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "4b63337f-b236-4033-bdcc-8066cbc64ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_schemas = [\n",
    "    ResponseSchema(name=\"key_insights\", description='3-5 bullet points summarizing key insights/outlook on the topic'),\n",
    "    ResponseSchema(name=\"key_drivers\", description='Main economic/politcal indicators driving the topic'),\n",
    "    ResponseSchema(name='risks', description='Potential risks associated with the topic'),\n",
    "    ResponseSchema(name='sentiment', description='Overall social sentiment (positive/negative/neutral with evidence) and degree of sentiment in percentage')    \n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instructions = output_parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "id": "f7b4269b-2450-4ed0-9757-f2e06f06f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_key = \"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\"\n",
    "\n",
    "def analyze_text(s_query, relevant_text):\n",
    "    \n",
    "    # initiate llm model\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Analyze the following news corpus regarding {query} and extract:\n",
    "    {format_instructions}\n",
    "    \n",
    "    Corpus:\n",
    "    {text}\n",
    "    \n",
    "    \"\"\")\n",
    "    #client = openai.OpenAI(api_key=openai_key)\n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        temperature=0.3,\n",
    "        openai_api_key=openai_key  # Pass key directly or use environment variable\n",
    "    )\n",
    "\n",
    "    messages = prompt.format_messages(\n",
    "        query = s_query,\n",
    "        text = relevant_text,\n",
    "        format_instructions=format_instructions\n",
    "    )\n",
    "\n",
    "    response = llm(messages)\n",
    "    return output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "id": "4b3e0e1f-6748-478e-bb57-47dd8a65ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_search_index(full_text):\n",
    "    # Split text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    chunks = text_splitter.split_text(full_text)\n",
    "\n",
    "    # creating searchable index\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_key)\n",
    "    return FAISS.from_texts(chunks, embeddings)    \n",
    "\n",
    "def analyze_with_semantic_search(s_query, text_list, n_results=10):\n",
    "    full_texts = \" \".join(text_list) if isinstance(text_list, list) else text_list\n",
    "    # creating vector index on full corpus\n",
    "    index = create_search_index(full_texts)\n",
    "\n",
    "    # retreiving relevant chunks\n",
    "    bm25_retriever = BM25Retriever.from_texts(full_texts)\n",
    "    faiss_retriever = index.as_retriever()\n",
    "    ensemble_retriever = EnsembleRetriever(\n",
    "        retrievers=[bm25_retriever, faiss_retriever], \n",
    "        weights=[0.4,0.6]\n",
    "    )\n",
    "    \n",
    "    relevant_text = ensemble_retriever.get_relevant_documents(s_query)\n",
    "    \n",
    "    return analyze_text(s_query, relevant_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "id": "f6501617-8951-4602-ab08-6a1ab9f66e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_insights': \"1. War with top trade partners impacting investor risk appetite. 2. Cryptocurrency trading in lockstep with stocks indicating uncertainty for the US currency. 3. European stocks rise on hopes of tariff pause by Trump administration. 4. NFT marketplace gaining traction as a one-stop shop for digital items. 5. Markets reacting to Trump's tariff war with caution.\", 'key_drivers': \"Trade tensions with top partners, Trump administration's tariff policies, cryptocurrency market behavior, European stock market trends, NFT marketplace adoption.\", 'risks': 'Potential risks include increased market volatility, uncertainty in currency markets, trade war escalation impacting global economy, and regulatory challenges for NFT marketplace.', 'sentiment': 'Overall sentiment is neutral with a slight negative bias due to concerns over trade tensions and market volatility. Degree of sentiment is 60% negative based on the cautious market behavior and uncertainties highlighted.'}\n"
     ]
    }
   ],
   "source": [
    "## reruning semantic search on data inclusive of reddit posts\n",
    "s_query = 'Impact of tarrifs'\n",
    "full_texts = cleaned_corpus\n",
    "result = analyze_with_semantic_search(s_query, full_texts, n_results=5)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "id": "45430c65-14cd-40dc-ba4a-2fb6cc7f39d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key_insights': '1. War with top trade partners impacting investor risk appetite. 2. Cryptocurrency trading in line with stocks indicating uncertainty for US currency. 3. European stocks rise on hopes of tariff pause by Trump administration. 4. NFT marketplace gaining traction for trading digital items. 5. Markets reacting to trade-war risks with volatility.', 'key_drivers': \"Trade tensions with top partners, US currency stability, Trump administration's tariff policies, NFT marketplace adoption, overall market volatility.\", 'risks': 'Potential risks include prolonged trade tensions impacting global economy, currency instability, market uncertainty due to tariff policies, and volatility in digital asset trading.', 'sentiment': \"Overall sentiment is neutral with a slight negative bias due to uncertainty and market volatility. Evidence from the corpus includes phrases like 'leaving investors unwilling to take on too much risk' and 'fast-evolving tariff war'. Sentiment degree is 40% negative.\"}\n"
     ]
    }
   ],
   "source": [
    "## reruning semantic search on data exclusive of reddit posts\n",
    "s_query_no_reddit = 'Impact of tarrifs'\n",
    "full_texts_no_reddit = cleaned_corpus\n",
    "result_2 = analyze_with_semantic_search(s_query_no_reddit, full_texts, n_results=5)\n",
    "\n",
    "print(result_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cbfb46-ee8c-4b2b-86f7-c7b092cd4505",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829fa0a-8e98-4f31-a1f7-23e6dac665bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "                                               "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
